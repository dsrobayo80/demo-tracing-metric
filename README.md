# Observability

## 1. Definition

### 1.1 Introduction

### - What is ?
In distributed architectures, observability represents a comprehensive approach that provides a detailed, real-time view of the behavior and state of a system. This is achieved through the collection and analysis of data generated by the system itself, which facilitates the management and reliability improvement of distributed systems, such as microservices. For a better understanding of this concept, we can break it down into three key components:

![key-components](src/main/resources/documentation/key-components.png)

- **Logs:** A log is an unalterable record that contains temporary information about a specific event that took place during the execution of a system or application. Through logs, we can obtain a detailed and functional view of what is happening in our system, as well as information about errors, warnings, or significant events.

- **Metrics:** These are quantitative data that provide information about the performance and behavior of the system at a specific point in time. These may include measurements such as response time, CPU usage, number of requests, among others. Metrics are valuable for evaluating the performance and efficiency of a system.

- **Traces:** They allow following the path of a request as it traverses different components of a distributed system. This is especially useful for identifying bottlenecks, understanding latency and understanding how services interact with each other.

It is important to understand that a trace (end-to-end of a request) is decomposed into what are called Spans, which are basically "execution blocks" that make up the trace.

In addition to the three pillars already mentioned, there is another essential concept that facilitates the interconnection between the different components of our system, and even with external systems: Context Propagation. This is the ability to transmit relevant information about a request or event along the execution flow through different components and services of a system. This gives us the ability to relate traces, logs and metrics effectively.

![context-propagation](src/main/resources/documentation/context-propagation.png)

### For what ?
Observability plays a key role in IT systems management by providing a detailed, real-time view of the behavior and performance of an application or infrastructure. Its main uses include:

- **Problem Identification and Resolution:** Enables development and operations teams to quickly detect and resolve problems and errors in a system, speeding up the resolution process and reducing downtime.

- **Performance Optimization:** Provides detailed information about an application's performance, enabling teams to identify areas of improvement and bottlenecks to optimize efficiency and speed of execution.

- **Traceability and Debugging:** Facilitates the tracking of an application or transaction as it traverses different components of a distributed system. This is essential for debugging and understanding execution logic, especially in complex environments.

- **Capacity Planning:** Allows teams to anticipate and allocate resources appropriately to handle future workloads. This avoids capacity issues and ensures optimal performance even in times of high demand.

- **User Experience Improvement:** Helps understand how user interactions affect the performance and availability of an application. This enables targeted improvements that lead to a more satisfying end-user experience.


### What is the benefits ?
Observability embodies formidable capabilities, offering a detailed, real-time view of a system's performance, along with the additional benefits described below:

- **Increased Reliability and Availability:** Observability helps prevent and mitigate problems before they affect end users, resulting in increased reliability and availability of services.

- **Rapid Problem Resolution:** Facilitates more efficient problem identification and resolution, reducing downtime and improving system reliability.

- **Resource Optimization:** Enables efficient resource allocation by identifying areas of over- or under-utilization, which can result in significant savings.

- **Continuous Improvement:** Facilitates real-time feedback and continuous improvement cycle by providing concrete data on system performance and efficiency.

- **Adaptive Capability:** Enables teams to respond quickly to changes in the environment and operating conditions, which is essential in dynamic and scalable environments.


### A brief introduction of this pattern in microservices

Observability in microservices uncovers unknown problems between various service interactions to build a resilient and secure application. You can adapt observability for your microservices through different design patterns. Six observability design patterns for microservices are briefly described below.

- **Distributed Tracing:**
  Distributed Tracing is a technique for tracing the flow of a request as it travels through different services in a distributed system. Each step is recorded as a "span" and correlated to form a complete trace.

- **Health Check API:**
  This API provides an access point for verifying the state of a service or application. It provides clear feedback on whether the service is functioning properly or if there are problems that require attention.

- **Log Aggregation:**
  This pattern involves the centralized collection, storage and analysis of logs generated by different components of an application or system. It facilitates the search and analysis of events relevant to observability.

- **Auditing:**
  Auditing involves the systematic recording of relevant safety and operational events. This allows the tracking of important activities, providing a layer of security and regulatory compliance in the system.

- **Exception Tracking:**
  This pattern focuses on the detection and logging of exceptions or errors in the application. It allows development teams to identify and resolve issues efficiently, maintaining system integrity and reliability.

- **Application Metrics:**
  Application metrics involve the collection and visualization of quantitative data about the performance and behavior of an application. This includes information such as response time, resource utilization and other key metrics that allow evaluating and optimizing system efficiency.


### Expectatives of differents actors like developers, consultants, clients or any other department.

#### - Developers:
- **Troubleshooting Efficiency:** Developers expect observability to provide accurate tools and data that facilitate rapid identification and resolution of code and infrastructure issues.

- **Performance Feedback:** They seek metrics and traces that allow them to understand how their code behaves in production, in order to optimize the performance and efficiency of their applications.

#### - Consultants and System Architects:**

- **Design and Optimization:** They look for observability tools that allow them to design scalable systems and optimize existing infrastructure based on customer needs.

- **Design and Architecture Validation:** They expect observability to confirm that the design and architecture decisions implemented translate into optimal performance and reliability.

#### - Customers:

- **Reliable Performance:** Customers want applications and services that run smoothly and efficiently, and they expect observability to ensure consistent performance and an uninterrupted user experience.

- **Transparency and Trust:** They want confidence that systems are being monitored and that any problems will be proactively detected and resolved before they affect their experience.

#### - Operations Teams:
- **Rapid Detection and Response:** Operations teams rely on observability to automatically detect performance issues or system outages and provide immediate alerts to take corrective action.

- **Capacity Planning:** They expect metrics data that allow them to effectively plan and allocate resources to handle load peaks without impacting performance.


#### - Security Department:
- **Threat Tracking and Detection:** they look for observability capabilities that allow them to track and detect malicious activity or suspicious behavior patterns in infrastructure and applications.

- **Compliance and Audit:** They expect observability to facilitate the collection of data and logs needed to comply with security and regulatory requirements.

#### - Sales and Marketing Department:
- **Quality Assurance and Customer Confidence:** They will value the ability to demonstrate to potential customers that the system is well monitored and that proactive steps are being taken to ensure reliable, high quality service.


### 1.2 Architecture

In this microservice with observability integration, the technologies work together to provide a complete and detailed view of system performance and behavior. **Micrometer** serves as the primary tool for collecting **metrics**, which are quantitative data that reflect the status and performance of the application. These metrics are exported to **Prometheus**, a monitoring and alerting platform, where they are stored and available for query. **Grafana** connects to Prometheus to visualize and analyze these metrics in the form of graphs and customized dashboards.

In addition to metrics, Micrometer also facilitates the export of execution traces to **Jaeger**, a traceability platform. These traces make it possible to follow the flow of a request as it traverses the different services of a distributed system. On the other hand, the **logs** generated by the application are sent to **Loki**, a log aggregation and query platform. This facilitates the search and analysis of events relevant to observability.

![architecture](src/main/resources/documentation/architecture.png)


### 1.3 Technologies

The effective implementation of observability in microservices environments requires the use of specialized tools that facilitate the collection, analysis and visualization of critical data. Some of the most commonly used tools for this purpose are listed below:

#### - OpenTelemetry:
An open source project that focuses on providing a standard and flexible way to instrument, collect and export observability data in distributed applications and systems. Its main goal is to enable development and operations teams to gain detailed insight into the performance, reliability and efficiency of their systems, especially in environments such as microservices.

#### - Micrometer:
Micrometer is a Java library designed to instrument applications and collect metrics from a variety of sources. Its integration with Open Telemetry allows data export to multiple observability systems, thus providing a unified view of metrics, traces and logs.
By combining Micrometer with Open Telemetry, applications can be instrumented to collect detailed metrics on the performance and operation of microservices. This includes information on latency, error rates and statistics related to code execution.

#### - Prometheus:
Prometheus is an open source monitoring and alerting system designed specifically for highly scalable and distributed environments. It offers a time-series based architecture and allows flexible querying and visualization of observability data.
In the context of microservices observability, Prometheus integrates effectively with Micrometer and Open Telemetry, enabling the collection and storage of long-term metrics. This provides teams with the ability to analyze trends and performance patterns over time.

#### - Grafana:
An open source data visualization platform that facilitates the creation of interactive dashboards and graphs for observability data. By connecting with Prometheus and other data sources, Grafana provides the ability to create custom visualizations and informative dashboards.
With Grafana, you can monitor and analyze metrics in real time, set threshold-based alerts, and visualize trends through an intuitive and highly configurable interface.

#### - Grafana Loki:
Grafana Loki is a powerful log aggregation and query tool designed specifically for distributed systems and microservices. Unlike traditional log management solutions, Loki uses a tag-oriented architecture that enables efficient search and query, even in highly scalable environments.

Its importance in microservice observability lies in its ability to centralize and analyze logs from multiple services, facilitating problem detection and event correlation in complex and highly dynamic environments. By enabling the rapid search and visualization of logs, Loki streamlines the problem identification and resolution process, which significantly contributes to the reliability and efficiency of microservices.

#### - Jaeger:
Jaeger is an open source traceability system that facilitates the capture, tracking and visualization of distributed transaction traces in microservices environments. It provides detailed information about the execution flow of requests across different services.
Jaeger's integration makes it possible to identify and analyze bottlenecks, optimize latency and understand the interaction between microservices. This is essential for improving the reliability and performance of distributed systems.

Together, these tools provide a complete and powerful platform for managing observability in microservice environments. By leveraging Micrometer with Open Telemetry, Prometheus, Grafana and Jaeger, teams can gain a complete and detailed view of the health and performance of their applications, facilitating informed decision making and efficient troubleshooting.

## 2. Implementation

### 2.1 Metrics

As mentioned above, metrics are quantitative data that provide information about the performance, behavior and status of a system or application. These metrics are essential for understanding how a system operates in real time and over time. Key concepts related to metrics in observability are presented below:

####  **Concepts:**

#### **Types of Metrics:**
- **Counters:** represent a value that can be incremented or decremented, but has no ordering relationship.

- **Gauges:** Represent a value that can vary in any direction, providing a snapshot of the current state.

- **Histograms:** Allow tracking of the distribution of values of a variable over a specific time interval.

- **Timers:** Measure the duration of events and provide information on latency and throughput.

#### **Sampling:**
Determine how often metrics are collected and recorded. Proper frequency is crucial to obtain accurate and meaningful data without causing an excessive burden on the system.

#### **Dimensions and Labels:**
They allow categorizing and segmenting metrics according to different attributes. This facilitates detailed analysis and the identification of specific patterns.

#### **Aggregation and Rollup:**
It consists of combining and reducing metrics at different levels of granularity. This facilitates the visualization and analysis of trends over time.

#### **Thresholds and Alerts:**
Establishing thresholds for metrics is essential to identify anomalous situations. Alerts are generated when a metric crosses or exceeds a predetermined threshold.

#### **Track Record and Trends:**
Maintaining a history of metrics over time is essential for retrospective analysis and identification of long-term patterns and trends.

#### **Visualization:**
Metrics are often presented in the form of graphs or charts for ease of interpretation and analysis. Visualization tools, such as Grafana, are essential to present metrics effectively.


####  **Good practices:**

- **Business Context:**
  Aligns metrics with business objectives and KPIs. Metrics should provide valuable and relevant information for business decision making.

- **Clear Meaning:**
  Make sure each metric has a clear purpose and well-defined meaning. Avoid ambiguous or generic metrics that do not add value.

- **Avoid Metric Overload:**
  Don't over-record metrics. Instead, focus on key metrics that actually provide valuable information for observability.


### 2.1 Logging

Logging in observability is a crucial practice for understanding and solving problems in complex systems. Here are the key concepts related to logging:

- **Severity Levels:**
  Logs typically have severity levels indicating the message's importance. Common levels include **DEBUG, INFO, WARNING, ERROR, and FATAL**.

*Best Practice:* Assigning appropriate severity levels to logs helps convey the importance and gravity of the event.

- **Structured vs. Unstructured Format:**
  Records can be structured, following a predefined, organized format (such as JSON), or unstructured, free text that may require more complex parsing.

*Best Practice:* Prefer structured formats like JSON or key-value pairs to enable automated search and analysis of records.

- **Timestamps:**
  Each record includes a timestamp indicating when the event occurred. This is crucial for event sequencing and temporal analysis.

*Best Practice:* Ensure each record has an accurate timestamp for seamless sequencing and temporal analysis.

- **Context and Metadata:**
  In addition to the message itself, logs often contain metadata like transaction identifiers, HTTP response codes, user IDs, etc., providing additional context for the event.

*Best Practice:* Add pertinent metadata, such as transaction identifiers and HTTP response codes, along with other contextual details, to enrich log information.

- **Log Retention:**
  To prevent storage space saturation, logs are regularly rotated or deleted after a specific duration. Retention policies dictate how long records are kept.

*Best Practice:* Implement rotation and retention policies to manage storage space effectively, ensuring the availability of pertinent historical records.

- **Shipping and Collection:**
  Records are sent to collection or aggregation systems, such as Grafana Loki, via specific agents or integrations.

- **Search and Query:**
  Tools such as Loki enable efficient search and query of records, facilitating problem detection and resolution.

### 2.3 Traces

In the complex environment of distributed systems, understanding the flow of a request through various components is critical to observability. This is achieved through the practice of traceability, which breaks down a request into units called "spans". These, in turn, provide a detailed view of the various execution blocks that make up the complete trace. In addition, traceability includes context propagation, which allows events and activities to be related between different parts of the flow. Through careful instrumentation and the application of best practices, teams can capture and analyze traces, making it easier to troubleshoot and optimize performance in complex, distributed environments.

####  **Concepts:**

- **Traceability Context:**
  Traceability context refers to information that is propagated across services to correlate related events. This allows a request to be tracked as it traverses different system components.

- **Instrumentation:**
  Instrumentation involves adding code to an application to capture traceability information, such as execution times and relationships between services.

- **Idempotency:**
  Idempotent operations are those that can be repeated without causing additional changes to the state of the system. This is crucial to ensure that traces are accurate and consistent.


####  **Good Practices:**

- **Set Unique Identifiers:**
  Assign unique identifiers to each request and propagate them across services. This facilitates trace correlation and dependency identification.

- **Add Meaningful Metadata:**
  Add relevant metadata, such as HTTP response codes and request details, to each span to enrich traceability information.

- **Minimize Performance Impact:**
  Implement efficient instrumentation techniques to ensure that trace capture does not introduce significant overhead to the application.

- **Hierarchize Spans:**
  Organize spans in a hierarchical structure that reflects the relationship between different system components. This facilitates the visualization and analysis of complex traces.

- **Use Traceability Standards:**
  Follow traceability standards such as OpenTracing or OpenTelemetry to ensure interoperability between different tools and services.

- **Configure Data Retention:**
  Establish data retention policies to ensure that traces are retained long enough for analysis and troubleshooting.

- **Validate and Corroborate Traces:**
  Regularly verify the accuracy and consistency of traces through automated testing and validation.

- **Monitor Tracing Latency:**
  Monitor the time it takes to capture and process traces to ensure that it does not adversely affect application performance.

### 2.4 Environments

#### 2.4.1 Development

Before diving into the code, it is essential to establish a solid observability infrastructure for our REST API. This will allow us to monitor and understand its performance in real time.

- To start an HTTP server, we have to choose the **org.springframework.boot:spring-boot-starter-web** dependency.

- To create observations using the **@Observed** aspect, we have to add the **org.springframework.boot:spring-boot-starter-aop** dependency.

- To add observation aspects, we must choose **spring-boot-starter-actuator** (to add Micrometer to the classpath).

######
After adding dependencies for running a web server and adding Micrometer support, it is time to add observability-related functions. To achieve this, we will integrate three key tools:

#### - Metrics:
- For Micrometer metrics with Prometheus, we need to add the **io.micrometer:micrometer-registry-prometheus** dependency.

#### - Tracing:
- To trace **context propagation** with Micrometer, we have to choose a bridge by adding the **io.micrometer:micrometer-tracing-bridge-otel** dependency.

- For latency display, we need to send the finished **spans** in some format to a server, for which the **io.opentelemetry:opentelemetry-exporter-otlp** dependency must be added.

#### - Logs:
- Loki will be used for log management, for which it is necessary to add the dependency **com.github.loki4j:loki-logback-appender** to send the logs to Loki (for versions see the following
  [link](https://central.sonatype.com/artifact/com.github.loki4j/loki-logback-appender?smo=true)).


#####
Based on the above, the following is the dependencies section of the POM file:

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>

    <!-- For Observation AOP -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-aop</artifactId>
    </dependency>

    <!--For Micrometer metrics with Prometheus-->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-registry-prometheus</artifactId>
        <scope>runtime</scope>
    </dependency>

    <!--For Tracing Context Propagation-->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-tracing-bridge-otel</artifactId>
    </dependency>

    <!--For tracing exporter-->
    <dependency>
        <groupId>io.opentelemetry</groupId>
        <artifactId>opentelemetry-exporter-otlp</artifactId>
    </dependency>

    <!--For Loki-->
    <dependency>
        <groupId>com.github.loki4j</groupId>
        <artifactId>loki-logback-appender</artifactId>
        <version>1.4.2</version>
    </dependency>
</dependencies>
```

######
Now we need to add some configuration. We add the settings in the properties file:
/src/main/resources/application.properties

```properties
server.port=8080
spring.application.name=client

# All traces should be sent to latency analysis tool
management.tracing.sampling.probability=1.0

management.endpoints.web.exposure.include=prometheus

# traceID and spanId are predefined MDC keys - we want the logs to include them
logging.pattern.level=%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]

# For Exemplars to work we need histogram buckets
management.metrics.distribution.percentiles-histogram.http.server.requests=true

logging.level.org.springframework.web=INFO

tracing.url=http://localhost:4317
```
######
The Loki Appender configuration looks exactly the same. /src/main/resources/logback-spring.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml" />
    <springProperty scope="context" name="appName" source="spring.application.name"/>

    <appender name="LOKI" class="com.github.loki4j.logback.Loki4jAppender">
        <http>
            <url>http://localhost:3100/loki/api/v1/push</url>
        </http>
        <format>
            <label>
                <pattern>app=${appName},host=${HOSTNAME},traceID=%X{traceId:-NONE},level=%level</pattern>
            </label>
            <message>
                <pattern>${FILE_LOG_PATTERN}</pattern>
            </message>
            <sortByTime>true</sortByTime>
        </format>
    </appender>

    <root level="INFO">
        <appender-ref ref="LOKI"/>
    </root>
</configuration>
```


## 3. Examples

In the next subsections, put examples specifying how to develop or add observability for differents cases. A simple REST API, detailed below, is taken as a reference:


service/MyUserService.java
```java
 @Service
public class MyUserService {

  private static final Logger log = LoggerFactory.getLogger(MyUserService.class);
  private final Random random = new Random();

  /**
   * Description: Example of using an annotation to observe methods <user.name> will be used as a
   * metric name <getting-user-name> will be used as a span name <userType=userType2> will be set as
   * a tag for both metric & span
   */
  @Observed(
      name = "user.name",
      contextualName = "getting-user-name"
      // lowCardinalityKeyValues = {"userType", "userType2"}
      )
  public String getuserName(String userId) {
    log.info("Getting user name for user with id <{}>", userId);
    try {
      Thread.sleep(random.nextLong(200L)); // simulates latency
    } catch (InterruptedException e) {
      throw new RuntimeException(e);
    }
    return "David-" + getNumber();
  }

  private int getNumber() {
    try {
      Thread.sleep(2000); // simulates latency
    } catch (InterruptedException e) {
      throw new RuntimeException(e);
    }
    return random.nextInt(999);
  }
}
```

controller/MyController.java
```java
@RestController
public class MyController {

  private static final Logger log = LoggerFactory.getLogger(MyController.class);
  private final MyUserService myUserService;

  MyController(MyUserService myUserService) {
    this.myUserService = myUserService;
  }

  @GetMapping("/user/{userId}")
  String userName(@PathVariable("userId") String userId) {
    log.info("Got a request");
    return myUserService.getuserName(userId);
  }
}
```

configuration/
 ```java
 @Configuration
public class MyConfiguration {
  @Bean
  RestTemplate restTemplate(RestTemplateBuilder builder) {
    return builder.build();
  }

  @Bean
  public OtlpGrpcSpanExporter otlpHttpSpanExporter(@Value("${tracing.url}") String url) {
    return OtlpGrpcSpanExporter.builder().setEndpoint(url).build();
  }
}

@Configuration(proxyBeanMethods = false)
class ObservedConfiguration {
  // To have the @Observed support we need to register this aspect
  @Bean
  public ObservedAspect observedAspect(ObservationRegistry observationRegistry) {
    return new ObservedAspect(observationRegistry);
  }
}
```

### 3.1 Integration with Prometheus
Below is the step-by-step to integrate the microservice with Prometheus using Docker.
In the following [Link](https://prometheus.io/docs/prometheus/latest/installation/) you will find the official documentation of the Prometheus installation process.


- The first step is to "Pull" the prom/prometheus image from docker:

```sh
$ docker pull prom/prometheus
```

- Now the downloaded image is executed using the following command:
``` sh
$ docker run -p 9090:9090 prom/prometheus
```

By performing the two previous steps you can access the link http://localhost:9090/ and see the Prometheus graphical interface, however, to connect it to our application, it is necessary to perform the following process:

- Save the following file as **prometheus.yml** in a known path:
```yml
global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s
alerting:
  alertmanagers:
  - follow_redirects: true
    enable_http2: true
    scheme: http
    timeout: 10s
    api_version: v2
    static_configs:
    - targets: []
scrape_configs:
- job_name: prometheus
  honor_timestamps: true
  scrape_interval: 10s
  scrape_timeout: 5s
  metrics_path: /actuator/prometheus
  scheme: http
  follow_redirects: true
  static_configs:
  - targets:
    - host.docker.internal:8080
```

**Note:** There are two important concepts in the file that allow the interfacing of the micro with Prometheus.:
- *metrics_path*: Where metrics are obtained from.
- *targets:* It's like localHost but in docker.

Now run the following command to run Prometheus but taking into account the **prometheus.yml** file configuration
```sh
$ docker run -p 9090:9090 -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus
```
**Note:** replace path/to with the known path where the **prometheus.yml** file was saved.

That way you can run the bug and plot the different metrics in Prometheus, an example of a custom metric is the one we had in the *getuserName()* method of the *MyUserService* class that we called **"user.name"** in the *@Observed*, in Prometheus it is translated as **"user_name"** and has different attributes.

![prometheus](src/main/resources/documentation/prometheus.png)

In the image above you can see the example of the graph related to the metric **"user_name_seconds_count"** with a function called **"increase"** which is updated under a time period of 1 minute.

### 3.2 Integration with Grafana
The integration with Grafana is similar to that of Prometheus since a docker image will be used.
In the following [Link](https://grafana.com/docs/grafana/latest/setup-grafana/installation/docker/) you will find the official documentation of the Grafana installation process.

- The first step is to "Pull" the grafana/grafana image from docker:
```sh
$ docker pull grafana/grafana
```

- Now the downloaded image is executed using the following command:
``` sh
$ docker run -d -p 3000:3000 --name=grafana -e "GF_INSTALL_PLUGINS=grafana-clock-panel, grafana-simple-json-datasource" grafana/grafana-enterprise
```
Now you must enter the link http://localhost:3000/, there it will ask you to register a username and password (you can choose a username and password of your choice) as this runs on your premises.

### Alternative
Because Grafana has integration with Prometehus and Loki, it is easier to use the following docker compose file that provides the necessary configuration for such integration.

```yml
version: "3.7"

networks:
  loki:

services:
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - loki

  promtail:
    image: grafana/promtail:2.9.0
    volumes:
      - /var/log:/var/log
    command: -config.file=/etc/promtail/config.yml
    networks:
      - loki

  grafana:
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /etc/grafana/provisioning/datasources
        cat <<EOF > /etc/grafana/provisioning/datasources/ds.yaml
        apiVersion: 1
        datasources:
        - name: Loki
          type: loki
          access: proxy
          orgId: 1
          url: http://loki:3100
          basicAuth: false
          isDefault: true
          version: 1
          editable: false
        
        - name: prometheus
          uid: prometheus
          type: prometheus
          access: proxy
          url: http://host.docker.internal:9090
          basicAuth: true #username: admin, password: admin
          basicAuthUser: admin
        EOF
        /run.sh
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    networks:
      - loki
```
**Note:** In the Prometheus server URL field, type http://host.docker.internal:9090/ as shown in the following image.

When you access the link http://localhost:3000/, you will see the following datasources:

![datasources](src/main/resources/documentation/datasources.png)

#### 3.2.1 Grafana Dashboard

- Prometheus: Below is the graph of a dashboard whose data source is Prometheus, which is similar to the graph in point 3.1 since it corresponds to the same metric in the same period of time.
  ![grafana](src/main/resources/documentation/grafana.png)


- Loki: The following table shows the filtered logs of the application in a period of time.
  ![grafana-Loki](src/main/resources/documentation/Grafana-Loki.png)


### 3.3 Integration with Jaeger

To integrate with Jaeger, you must have this docker-compose file on the microservice:

```yml
version: '3.7'
services:

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686" # the jaeger UI port
      - "4317:4317" # the open telemetry grpc port for sending traces
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=debug
```

And run the following command to run the service
```sh
$ docker-compose up
```

Now, you can access the link http://localhost:16686 and visualize Jaeger, on the left side a section of services is displayed and there you can choose **"Client"** which was the name I gave to my application in the .properties file and you will see the traces of the requests made to the microservice, an example of these traces is shown below:

![Jaeger](src/main/resources/documentation/Jaeger.png)

In the image you can see the trace with two spans, the first one from the request to the Controller and the second one from my method in the *@Service*. So, if you want to add observability to other methods it is necessary to add the *@Observed* annotation to be displayed in **Jaeger**, **Prometheus** and **Grafana**.

### 3.4 Integration with AWS Cloud Watch

### 3.5 Integration with AWS XRay

### 3.6 Integration with Azure Monitoring
